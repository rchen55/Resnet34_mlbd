{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aacaa3e-d01b-4d7e-8208-b437f0326d0d",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44970324-c3cb-4d8c-a3ad-f5625fb24e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf8b8311-c781-4848-85df-6eb84f9c2aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2536 images and 2536 masks\n",
      "Found 1088 images and 1088 masks\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SegmentationDataset' object has no attribute 'image_masks_pairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SegmentationDataset(root_dir\u001b[38;5;241m=\u001b[39mroot_dir, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[1;32m    101\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SegmentationDataset(root_dir\u001b[38;5;241m=\u001b[39mroot_dir, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[0;32m--> 103\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m    104\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m#Debugging\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/global/lib/python3.12/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/global/lib/python3.12/site-packages/torch/utils/data/sampler.py:142\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/global/lib/python3.12/site-packages/torch/utils/data/sampler.py:149\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_source)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "Cell \u001b[0;32mIn[12], line 71\u001b[0m, in \u001b[0;36mSegmentationDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_masks_pairs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SegmentationDataset' object has no attribute 'image_masks_pairs'"
     ]
    }
   ],
   "source": [
    "INPUT_IMAGE_HEIGHT = 1024\n",
    "INPUT_IMAGE_WIDTH = 672\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "            target = t(target)\n",
    "        target = torch.tensor(np.array(target), dtype=torch.int64)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, phase='Train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "\n",
    "        sub_dirs = ['Brown_Field', 'Main_Trail', 'Power_Line', 'mixed']\n",
    "\n",
    "        for sub_dir in sub_dirs:\n",
    "            subdir_path = os.path.join(root_dir, sub_dir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                img_dir = os.path.join(subdir_path, phase, 'imgs')\n",
    "                mask_dir = os.path.join(subdir_path, phase, 'annos', 'int_maps')\n",
    "\n",
    "                if os.path.exists(img_dir) and os.path.exists(mask_dir):\n",
    "                    img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "                    mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "\n",
    "                    if len(img_files) == 0 or len(mask_files) == 0:\n",
    "                        print(f\"No images or masks found in {img_dir} or {mask_dir}\")\n",
    "                    else:\n",
    "                        for img_file, mask_file in zip(img_files, mask_files):\n",
    "                            self.image_paths.append(os.path.join(img_dir, img_file))\n",
    "                            self.mask_paths.append(os.path.join(mask_dir, mask_file))\n",
    "                else:\n",
    "                    print(f\"Image directory {img_dir} or mask directory {mask_dir} does not exist\")\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            print(\"No images or masks found in any subdirectory\")\n",
    "        else:\n",
    "            print(f\"Found {len(self.image_paths)} images and {len(self.mask_paths)} masks\")\n",
    "        \n",
    "    def _collect_image_mask_pairs(self):\n",
    "        image_mask_pairs = []\n",
    "        for root_dir in self.root_dirs:\n",
    "            image_dir = os.path.join(root_dir, self.split, 'imgs')\n",
    "            mask_dir = os.path.join(root_dir, self.split, 'annos', 'int_maps')\n",
    "            \n",
    "            images = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswidth('.png')])\n",
    "            masks = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswidth('.png')])\n",
    "            \n",
    "            mask_dict = {os.path.basename(mask).split('_')[1].replace('.png',''): mask for mask in masks}\n",
    "            for img in images:\n",
    "                key = os.path.basename(img).split('_')[1].replace('.png', '')\n",
    "                if key in mask_dict:\n",
    "                    image_mask_pairs.append((img, mask_dict,[key]))\n",
    "                else:\n",
    "                    print(f\"No matching mask for image: {img}\")\n",
    "                    \n",
    "        return image_mask_pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_masks_pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, mask_path = self.image_mask_pairs[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask - Image.open(mask_path).convert(\"L\")\n",
    "        if self.transforms:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "        return image, mask\n",
    "    \n",
    "#Transformation definition\n",
    "transform = Compose([transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH), interpolation=Image.NEAREST)])\n",
    "\n",
    "#List of root directories\n",
    "root_dirs = ['CAT/Brown_Field', 'CAT/Main_Trail', 'CAT/Power_Line', 'CAT/mixed']\n",
    "\n",
    "#Data Transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_transforms = transforms.Compose([\n",
    "    \n",
    "])\n",
    "\n",
    "#Data loader\n",
    "root_dir = '/home/rchen2/CAT'\n",
    "\n",
    "train_dataset = SegmentationDataset(root_dir=root_dir, phase='Train', transform=data_transforms)\n",
    "test_dataset = SegmentationDataset(root_dir=root_dir, phase='Test', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=4, shuffle=True,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,)\n",
    "\n",
    "#Debugging\n",
    "for images, masks in train_loader:\n",
    "    print(f\"Batch of images shape: {images.shape}\")\n",
    "    print(f\"Batch of masks shape: {masks.shape}\")\n",
    "    break\n",
    "\n",
    "#Instantiate the model\n",
    "model = smp.Unet(encoder_name = \"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=4).cuda()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16461afe-e9ca-4ee0-bd5a-821e83df0bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#define criterion and optimizer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     10\u001b[0m NUM_CLASSES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTrainer\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, num_classes, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train.loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.train_iou_per_class = []\n",
    "        \n",
    "    def calculate_iou(self, pred, target):\n",
    "        iou_per_class = []\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        \n",
    "        for cls in range(self.num_classes):\n",
    "            pred_inds = (pred == cls)\n",
    "            target_inds = (target == cls)\n",
    "            \n",
    "            intersection = (pred_inds & target_inds).sum().item()\n",
    "            union = (pred_inds | target_inds).sum().item()\n",
    "            \n",
    "            if union == 0:\n",
    "                iou_per_class.append(float('nan'))\n",
    "            else:\n",
    "                iou_per_class.append(intersection / union)\n",
    "                \n",
    "        return iou_per_class\n",
    "\n",
    "    def mean_iou(self, iou_list):\n",
    "        valid_iou = [iou for iou in iou_list if not np.isnan(iou)]\n",
    "        if len(valid_iou) == 0:\n",
    "            return float('nan')\n",
    "        return np.mean(valid_iou)\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_iou_scores = np.zeros((len(self.train_loader), self.num_classes))\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(self.train_loader, desc=f'Training Epoch {epoch}')):\n",
    "            images = images.to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "            \n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, masks)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #Calculate IoU\n",
    "            iou = self.calculate_iou(outputs, masks)\n",
    "            epoch_iou_scores[batch_idx] = iou\n",
    "            \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        avg_iou_scores = np.nanmean(epoch_iou_scores, axis=0)\n",
    "        self.train_iou_per_class.append(avg_iou_scores)\n",
    "        \n",
    "        print(f\"Epoch [{epoch}] Training Loss: {avg_loss}\")\n",
    "        print(f\"Epoch [{epoch}] IoU per class: {avg_iou_scores}\")\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        iou_scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(self.val_loader, desc='Validating'):\n",
    "                images = images.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                #Calculate IoU\n",
    "                iou = self.calculate_iou(outputs, masks)\n",
    "                epoch_iou_scores[batch_idx] = iou\n",
    "                \n",
    "            avg_loss = running_loss / len(self.train_loader)\n",
    "            \n",
    "            #Compute mean IoU\n",
    "            iou_scores = np.array(iou_scores)\n",
    "            miou_per_class = np.nanmean(iou_scores, axis=0)\n",
    "            miou = self.mean_iou(miou_per_class)\n",
    "            \n",
    "            print(f\"Validation Loss: {avg_loss}\")\n",
    "            print(f\"Mean IoU: {miou}\")\n",
    "            print(f\"IoU per class: {miou_per_class}\")\n",
    "            return avg_loss, miou, miou_per_class\n",
    "        \n",
    "        def train(self, num_epochs):\n",
    "            for epoch in range(1, num_epochs + 1):\n",
    "                self.train_epoch(epoch)\n",
    "                self.validate_epoch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0e489-2eba-41e0-a2b5-5178a9f80ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define class weights based on the dataset\n",
    "class_weights = torch.tensor([1.0, 2.0, 2.0, 2.0]).cuda()\n",
    "\n",
    "#Use weighted loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "#Instantiate the Trainer class with weighted loss\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "num_epochs=4\n",
    "trainer.train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441f756-ed69-46a1-8579-f11d24805fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_class_distribution(loader, num_classes):\n",
    "    class_counts = [0] * num_classes\n",
    "    \n",
    "    for _, masks in loader:\n",
    "        for mask in masks:\n",
    "            class_counts[cls] += (mask == cls).sum().item()\n",
    "            \n",
    "    total_pixels = sum(class_counts)\n",
    "    class_distribution = [count / total_pixels for count in class_counts]\n",
    "    \n",
    "    print(f\"Class distribution: {class_distribution}\")\n",
    "    \n",
    "print_class_distribution(train_loader, NUM_CLASSES)\n",
    "print_class_distribution(test_loader, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29f1fc-5f96-4dd9-be5c-ed5e189237c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plotting IoU per class\n",
    "def plot_iou_per_class(trainer, num_epochs, num_classes):\n",
    "    epochs = np.arrange(1, num_epochs + 1)\n",
    "    iou_per_class = np.array(trainer.train_iou_per_class).T\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    classNames = ['Background', 'Sedan', 'Pickup', 'Off-Road']\n",
    "    for cls in range(num_classes):\n",
    "        plt.plot(epochs, iou_per_class[cls], label=f'{classNames[cls]} IoU')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.title('ResNet-34')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "plot_iou_per_class(trainer, num_epochs, NUM_CLASSES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLOBAL",
   "language": "python",
   "name": "global"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
